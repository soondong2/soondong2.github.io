---
title: "[Tensorflow] Model Compile"
date: 2023-03-06

categories:
  - AI
  - Deep Learning
tags:
  - DL
  - Tensorflow
---
    

## 모델 컴파일(compile)
모델을 구성한 후 사용할 `손실 함수(loss function)`와, `옵티마이저(optimizer)`를 지정한다.


```python
from tensorflow.keras.layers import Dense, Activation, Flatten, Input
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.utils import plot_model
```


```python
# 모델 생성
inputs = Input(shape=(28, 28, 1))
x = Flatten(input_shape=(28, 28, 1))(inputs)
x = Dense(units=300, activation='relu')(x)
x = Dense(units=100, activation='relu')(x)
x = Dense(units=10, activation='softmax')(x)

model = Model(inputs=inputs, outputs=x)
model.summary()
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_3 (InputLayer)        [(None, 28, 28, 1)]       0         
                                                                     
     flatten_2 (Flatten)         (None, 784)               0         
                                                                     
     dense_6 (Dense)             (None, 300)               235500    
                                                                     
     dense_7 (Dense)             (None, 100)               30100     
                                                                     
     dense_8 (Dense)             (None, 10)                1010      
                                                                     
    =================================================================
    Total params: 266,610
    Trainable params: 266,610
    Non-trainable params: 0
    _________________________________________________________________
    


```python
# 모델 컴파일
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
```

## 손실 함수(Loss Function)
- 학습이 진행되면서 해당 과정이 얼마나 잘 되고 있는지를 나타내는 지표
- 모델이 훈련되는 동안 최소화될 값으로 주어진 문제에 대한 성공 지표
- 즉, 예측값과 실제값의 차이를 손실 함수를 통해 계산
- 손실 함수에 따른 결과를 통해 학습 파라미터를 조정
- 최적화 이론에서 최소화 하고자 하는 함수
- 미분 가능한 함수 사용

### 분류
- `sparse_categorical_crossentropy` : 클래스가 배타적 방식으로 구분 (0, 1, 2, ... , 9)
- `categorical_cross_entropy` : 클래스가 원-핫 인코딩 방식으로 되어 있을 경우 사용
- `binary_crossentropy` : 이진 분류

#### 원-핫 인코딩(One-Hot Encoding)
- 범주형 변수를 표현할 때 사용
- 가변수(Dummy Variable)이라고도 함
- 정답인 레이블을 제외하고 모두 0으로 처리 (정답은 1)

#### 교차 엔트로피 오차(Cross Entropy Error, CEE)
- 이진 분류(Binary Classification), 다중 클래스 분류(Multi Class Classification)
- 소프트맥스(softmax)와 원-핫 인코딩 사이의 출력 간 거리
- 정답인 클래스에 대해서만 오차를 계산
- 정답을 맞추면 0, 틀리면 그 차이가 클수록 오차가 무한히 커짐

#### 이진 분류 문제의 교차 엔트로피(Binary Cross Entropy, BCE)
- 이진 분류 문제에서 크로스 엔트로피 오차를 손실 함수로 사용

### 회귀
#### 평균절대오차(Mean Absolute Error, MAE)
- 오차가 커져도 손실 함수가 일정하게 증가
- 이상치에 강건함

#### 평균제곱오차(Mean Squared Error, MSE)
- 가장 많이 쓰이는 손실 함수 중 하나
- 오차가 커질수록 손실 함수가 빠르게 증가 (제곱)
- 정답과 예측한 값의 차이가 클수록 더 많은 패널티를 부여

## 옵티마이저(Optimizer)
- 손실 함수를 기반으로 모델이 어떻게 업데이트 되어야 할지를 결정
- `keras.optimizer.SGD()` : 확률적 경사 하강법
- `keras.optimizer.Adam()` : 자주 사용되는 옵티마이저

### 경사하강법(Gradient Descent)
- 미분과 기울기 (스칼라를 벡터로 미분한 것)
- 즉, **미분 값이 0인 지점**을 찾는 방법

### 학습률(Learning rate)
- 적절한 학습률을 지정해야 최저점에 잘 도달할 수 있음
- 학습률이 너무 크면 발산
- 학습률이 너무 작으면 학습이 오래 걸리거나 최저점에 도달하지 못함
