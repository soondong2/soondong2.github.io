---
title: "[PyTorch] Tensor Manipulation1"
date: 2023-02-28

categories:
  - AI
  - Deep Learning
tags:
    - DL
    - PyTorch
---
## 파이토치(PyTorch)

* 페이스북이 초기 루아(Lua) 언어로 개발된 토치(Torch)를 파이썬 버전으로 개발하여 2017년도에 공개
* 초기에 토치(Torch)는 넘파이(NumPy) 라이브러리처럼 과학 연산을 위한 라이브러리로 공개
* 이후 GPU를 이용한 텐서 조작 및 동적 신경망 구축이 가능하도록 딥러닝 프레임워크로 발전시킴
* 파이썬답게 만들어졌고, 유연하면서도 가속화된 계산 속도를 제공

## PyTorch 모듈 구조
![image](https://user-images.githubusercontent.com/100760303/221390292-8375fab4-8d2b-4df1-af91-79f3321a8d61.png)

## PyTorch 구성요소
- `torch`: 메인 네임스페이스, 텐서 등의 다양한 수학 함수가 포함
- `torch.autograd`: 자동 미분 기능을 제공하는 라이브러리
- `torch.nn`: 신경망 구축을 위한 데이터 구조나 레이어 등의 라이브러리
- `torch.multiprocessing`: 병럴처리 기능을 제공하는 라이브러리
- `torch.optim`: SGD(Stochastic Gradient Descent)를 중심으로 한 파라미터 최적화 알고리즘 제공
- `torch.utils`: 데이터 조작 등 유틸리티 기능 제공
- `torch.onnx`: ONNX(Open Neural Network Exchange), 서로 다른 프레임워크 간의 모델을 공유할 때 사용

## 텐서(Tensors)
* **데이터 표현을 위한 기본 구조**로 `텐서(tensor)`를 사용
* 텐서는 데이터를 담기위한 `컨테이너(container)`로서 일반적으로 수치형 데이터를 저장
* 넘파이(NumPy)의 `ndarray`와 유사
* GPU를 사용한 연산 가속 가능

![image](https://user-images.githubusercontent.com/100760303/221390304-0e95ad61-a797-4f84-bec6-a882c7a5073c.png)

## 텐서 조작(Tensor Manipulation)
- 텐서(Tensor)
- 넘파이(NumPy)
- 텐서 조작(Tensor Manipulation)
- 브로드캐스팅(Broadcasting)


```python
import numpy as np
import torch
```

## NumPy

1D Array with NumPy


```python
t = np.array([0., 1., 2., 3., 4., 5., 6.])
print(t)
```

    [0. 1. 2. 3. 4. 5. 6.]
    


```python
print('Rank of t :', t.ndim) # 몇 개의 차원? (Vector or Matrix or Tensor)
print('Sahpe of t' , t.shape) # 차원에 대한 element
```

    Rank of t : 1
    Sahpe of t (7,)
    


```python
print(t[0], t[1], t[-1]) # Element
print(t[2:5]) # Slicing
print(t[:2], t[3:]) # Slicing
```

    0.0 1.0 6.0
    [2. 3. 4.]
    [0. 1.] [3. 4. 5. 6.]
    

2D Array with NumPy


```python
t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])
print(t) # 4 x 3
```

    [[ 1.  2.  3.]
     [ 4.  5.  6.]
     [ 7.  8.  9.]
     [10. 11. 12.]]
    


```python
# 2개의 차원(Matrix)은 각각의 차원에 4 x 3로 이루어져 있음
print('Rank of t :', t.ndim) # 몇 개의 차원? (Vector or Matrix or Tensor)
print('Sahpe of t' , t.shape) # 차원에 대한 element
```

    Rank of t : 2
    Sahpe of t (4, 3)
    

## PyTorch

1D Array with PyTorch


```python
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)
```

    tensor([0., 1., 2., 3., 4., 5., 6.])
    


```python
print(t.dim()) # Rank
print(t.shape) # Shape
print(t.size()) # Shape

print(t[0], t[-1]) # Element
print(t[2:5]) # Slocing
print(t[:2]) # Slicing
```

    1
    torch.Size([7])
    torch.Size([7])
    tensor(0.) tensor(6.)
    tensor([2., 3., 4.])
    tensor([0., 1.])
    

2D Arrya with PyTorch


```python
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12]
                       ])
print(t)
```

    tensor([[ 1.,  2.,  3.],
            [ 4.,  5.,  6.],
            [ 7.,  8.,  9.],
            [10., 11., 12.]])
    


```python
print(t.dim()) # Rank
print(t.shape) # Shape
print(t.size()) # Shape

print(t[0], t[-1]) # Element
print(t[2:5]) # Slocing
print(t[:2]) # Slicing
```

    2
    torch.Size([4, 3])
    torch.Size([4, 3])
    tensor([1., 2., 3.]) tensor([10., 11., 12.])
    tensor([[ 7.,  8.,  9.],
            [10., 11., 12.]])
    tensor([[1., 2., 3.],
            [4., 5., 6.]])
    

## Broadcasting


```python
# Same shape
m1 = torch.FloatTensor([[3, 3]])
m2 = torch.FloatTensor([[2, 2]])
print(m1 + m2)
```

    tensor([[5., 5.]])
    


```python
# Vector + Scalar
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3]) # 3 -> [[3, 3]]
print(m1 + m2)
```

    tensor([[4., 5.]])
    


```python
# (2 x 1 Vector) + (1 x 2 Vector)
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([[3], [4]])
print(m1 + m2)
```

    tensor([[4., 5.],
            [5., 6.]])
    

## Multiplication vs Matrix Multiplication


```python
print('--------------')
print('Mul vs Matmul')
print('--------------')
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Sahpe of Matrix 2: ', m2.shape) # 2 x 1
```

    --------------
    Mul vs Matmul
    --------------
    Shape of Matrix 1:  torch.Size([2, 2])
    Sahpe of Matrix 2:  torch.Size([2, 1])
    


```python
print(m1 * m2)
print(m1.mul(m2))
```

    tensor([[1., 2.],
            [6., 8.]])
    tensor([[1., 2.],
            [6., 8.]])
    

## Mean
정수형(LongTensor)에서는 Mean을 사용할 수 없다.

```python
t = torch.FloatTensor([1, 2])
print(t.mean())
```

    tensor(1.5000)
    


```python
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)
```

    tensor([[1., 2.],
            [3., 4.]])
    


```python
print(t.mean())
print(t.mean(dim=0)) # 2 x 2 -> 1 x 2
print(t.mean(dim=1)) # 2 x 2 -> 2 x 1
```

    tensor(2.5000)
    tensor([2., 3.])
    tensor([1.5000, 3.5000])
    

## Sum


```python
print(t.sum())
print(t.sum(dim=0)) # 2 x 2 -> 1 x 2
print(t.sum(dim=1)) # 2 x 2 -> 2 x 1
```

    tensor(10.)
    tensor([4., 6.])
    tensor([3., 7.])
    

## Max and Argmax


```python
print(t.max())
```

    tensor(4.)
    


```python
print(t.max(dim=0))
print('Max : ', t.max(dim=0)[0])
print('Argmax : ', t.max(dim=0)[1])
```

    torch.return_types.max(
    values=tensor([3., 4.]),
    indices=tensor([1, 1]))
    Max :  tensor([3., 4.])
    Argmax :  tensor([1, 1])
    


```python
print(t.max(dim=1))
```

    torch.return_types.max(
    values=tensor([2., 4.]),
    indices=tensor([1, 1]))

